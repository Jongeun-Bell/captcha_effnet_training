"""
ê°œì„ ëœ í•™ìŠµ ì½”ë“œ (CAPTCHA ë°ì´í„°ìš©) + MLflow ì—°ë™
- Dataset í•œ ë²ˆë§Œ ìƒì„± í›„ random_splitë¡œ ë‚˜ëˆ„ê³  transform ë¶„ë¦¬ ì ìš©
- MLflow: log_model(name=..) ì ìš©, log_artifact ì¤‘ë³µ ì œê±°
- Model Signature ì¶”ê°€
"""

import torch
from torch import nn, optim
from torchvision import transforms, models
from torch.utils.data import DataLoader, random_split
import time
import os

import mlflow
import mlflow.pytorch
from mlflow.models.signature import infer_signature
from datetime import datetime

from captcha_dataset import CAPTCHADataset

# ============================================
# ì„¤ì •
# ============================================
data_dir = "/Users/bell/Desktop/captcha/images/"
output_dir = "./"
model_save_path = os.path.join(output_dir, "best_model.pth")

# ë°ì´í„°ì…‹ ë¶„í•  ë¹„ìœ¨
train_ratio = 0.8
val_ratio = 0.2

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
batch_size = 64
learning_rate = 0.0003
num_epochs = 20
patience = 3

# MLflow ì‹¤í—˜ ì´ë¦„
mlflow.set_experiment("captcha-effnet")

# MLflow Run ì´ë¦„ (LR, BS, ì‹œê°„ í¬í•¨)
run_name = f"effnet_lr{learning_rate}_bs{batch_size}_{datetime.now().strftime('%Y%m%d_%H%M')}"

# ============================================
# ì „ì²˜ë¦¬ ì •ì˜
# ============================================
train_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# ============================================
# ë°ì´í„° ë¡œë“œ
# ============================================
print("=" * 60)
print("ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
print("=" * 60)

# Dataset í•œ ë²ˆë§Œ ìƒì„± (transform=None)
full_dataset = CAPTCHADataset(data_dir, transform=None)

total_size = len(full_dataset)
train_size = int(total_size * train_ratio)
val_size = total_size - train_size

# random_split â†’ Subset ë‘ ê°œ ìƒì„±
train_subset, val_subset = random_split(full_dataset, [train_size, val_size])

# Subset ë‚´ë¶€ì—ì„œ ì°¸ì¡°í•˜ëŠ” dataset(transform)ì„ ê°ê° ì§€ì •
train_subset.dataset.transform = train_transform
val_subset.dataset.transform = val_transform

# DataLoader ìƒì„±
train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=0)
val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=0)

print(f"ì´ ë°ì´í„°: {total_size}ê°œ")
print(f"í•™ìŠµ ë°ì´í„°: {train_size}ê°œ")
print(f"ê²€ì¦ ë°ì´í„°: {val_size}ê°œ")
print(f"ë°°ì¹˜ í¬ê¸°: {batch_size}\n")

# ============================================
# ëª¨ë¸ ì„¤ì •
# ============================================
print("=" * 60)
print("ëª¨ë¸ ì„¤ì • ì¤‘...")
print("=" * 60)

device = "mps" if torch.backends.mps.is_available() else \
         "cuda" if torch.cuda.is_available() else "cpu"
print(f"ì‚¬ìš© ì¥ì¹˜: {device}\n")

model = models.efficientnet_b0(weights="IMAGENET1K_V1")

# ë¶„ë¥˜ì¸µ ìˆ˜ì • (2 í´ë˜ìŠ¤: Animal / Object)
in_features = model.classifier[1].in_features
model.classifier[1] = nn.Linear(in_features, 2)

model.to(device)
print("âœ… EfficientNet-B0 ë¡œë“œ ì™„ë£Œ (2ê°œ í´ë˜ìŠ¤)\n")

# ============================================
# ì†ì‹¤ í•¨ìˆ˜ / ì˜µí‹°ë§ˆì´ì € / ìŠ¤ì¼€ì¤„ëŸ¬
# ============================================
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)

# ============================================
# MLflow Run ì‹œì‘
# ============================================
print("=" * 60)
print("í•™ìŠµ ì‹œì‘")
print("=" * 60 + "\n")

start_total = time.time()
best_accuracy = 0
patience_counter = 0

# Signature ì¤€ë¹„ìš© ì˜ˆì‹œ ì…ë ¥ (128x128 ì´ë¯¸ì§€ 1ê°œ)
example_input = torch.randn(1, 3, 128, 128).to(device)
with torch.no_grad():
    example_output = model(example_input)
signature = infer_signature(example_input.cpu().numpy(),
                            example_output.cpu().numpy())

with mlflow.start_run(run_name=run_name):

    # íŒŒë¼ë¯¸í„° ë¡œê¹…
    mlflow.log_param("run_name", run_name)
    mlflow.log_param("batch_size", batch_size)
    mlflow.log_param("learning_rate", learning_rate)
    mlflow.log_param("num_epochs", num_epochs)
    mlflow.log_param("patience", patience)
    mlflow.log_param("train_samples", train_size)
    mlflow.log_param("val_samples", val_size)

    # ==========================
    # Epoch Loop
    # ==========================
    for epoch in range(num_epochs):
        epoch_start = time.time()
        
        # ----- í•™ìŠµ -----
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for batch_idx, (imgs, labels) in enumerate(train_loader):
            imgs, labels = imgs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()

            if (batch_idx + 1) % 10 == 0:
                print(f"  [{batch_idx+1}/{len(train_loader)}] Loss: {loss.item():.4f}")

        avg_train_loss = train_loss / len(train_loader)
        train_accuracy = train_correct / train_total

        # ----- ê²€ì¦ -----
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for imgs, labels in val_loader:
                imgs, labels = imgs.to(device), labels.to(device)
                outputs = model(imgs)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = val_correct / val_total

        scheduler.step()
        epoch_time = time.time() - epoch_start

        # ----- ì¶œë ¥ -----
        print(f"[Epoch {epoch+1}/{num_epochs}]")
        print(f"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2%}")
        print(f"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_accuracy:.2%}")
        print(f"  â†’ Epoch Time: {epoch_time:.2f} sec")

        # ----- ê³¼ì í•© ì²´í¬ (ì°¸ê³ ìš© ì¶œë ¥ë§Œ) -----
        overfitting_gap = train_accuracy - val_accuracy
        if overfitting_gap >= 0.10:
            print(f"  âš ï¸ ê³¼ì í•© ê²½ê³ ! (ì°¨ì´: {overfitting_gap:.2%})")
        else:
            print(f"  âœ“ ì •ìƒ (ì°¨ì´: {overfitting_gap:.2%})")

        # MLflow ê¸°ë¡
        mlflow.log_metric("train_loss", avg_train_loss, step=epoch)
        mlflow.log_metric("train_acc", train_accuracy, step=epoch)
        mlflow.log_metric("val_loss", avg_val_loss, step=epoch)
        mlflow.log_metric("val_acc", val_accuracy, step=epoch)
        mlflow.log_metric("learning_rate", optimizer.param_groups[0]['lr'], step=epoch)

        # ----- ğŸ”¥ í‘œì¤€ Early Stopping ë¡œì§ -----
        # 1) val_accuracyê°€ ì´ì „ bestë³´ë‹¤ í¬ë©´:
        #    - best_accuracy ì—…ë°ì´íŠ¸
        #    - patience_counter ë¦¬ì…‹
        #    - ëª¨ë¸ ì €ì¥ + MLflowì— log_model
        # 2) ì•„ë‹ˆë¼ë©´:
        #    - patience_counter ì¦ê°€
        #    - patienceë§Œí¼ ì—°ì†ìœ¼ë¡œ ê°œì„ ì´ ì—†ìœ¼ë©´ Early Stopping
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            patience_counter = 0

            torch.save(model.state_dict(), model_save_path)
            print(f"  âœ… ìµœê³  ëª¨ë¸ ì €ì¥! (Val_Acc: {val_accuracy:.2%})")

            mlflow.pytorch.log_model(
                model,
                name="best_model",
                signature=signature
            )
        else:
            patience_counter += 1
            print(f"  â³ Patience: {patience_counter}/{patience}")

            if patience_counter >= patience:
                print("\nğŸ›‘ Early Stopping ë°œë™!")
                print(f"ìµœê³  ê²€ì¦ ì •í™•ë„: {best_accuracy:.2%}")
                break

        print()

    mlflow.log_metric("best_val_acc", best_accuracy)

# ============================================
# í•™ìŠµ ì™„ë£Œ ì¶œë ¥
# ============================================
end_total = time.time()
print("=" * 60)
print("í•™ìŠµ ì™„ë£Œ!")
print(f"ìµœê³  ê²€ì¦ ì •í™•ë„: {best_accuracy:.2%}")
print(f"ì „ì²´ í•™ìŠµ ì‹œê°„: {end_total - start_total:.2f} sec")
print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_save_path}")
print("=" * 60)

# ============================================
# ìµœì¢… ëª¨ë¸ ê²€ì¦
# ============================================
print("\nìµœê³  ëª¨ë¸ ë¡œë“œ ì¤‘...")
model.load_state_dict(torch.load(model_save_path, map_location=device))
model.eval()

final_correct = 0
final_total = 0

with torch.no_grad():
    for imgs, labels in val_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        outputs = model(imgs)
        _, predicted = torch.max(outputs, 1)
        final_total += labels.size(0)
        final_correct += (predicted == labels).sum().item()

final_accuracy = final_correct / final_total
print(f"ìµœì¢… ê²€ì¦ ì •í™•ë„: {final_accuracy:.2%}")
print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ:", model_save_path)
